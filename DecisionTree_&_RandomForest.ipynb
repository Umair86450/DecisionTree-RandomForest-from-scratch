{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üå≥ Decision Tree Classifier (Python Implementation ‚Äì No Libraries)\n",
        "\n",
        "## üìå What is a Decision Tree?\n",
        "A decision tree is a flowchart-like structure that helps make decisions based on input features.  \n",
        "It splits the data into branches based on conditions until a final decision (class label) is reached.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why Use a Decision Tree?\n",
        "\n",
        "| Reason | Description |\n",
        "|--------|-------------|\n",
        "| üîç Easy to Understand | Works like human decisions: \"If condition, then result\" |\n",
        "| üß† No Need for Math Knowledge | Doesn't require linear algebra or calculus |\n",
        "| üîÄ Handles Mixed Data | Works with both numerical and categorical features |\n",
        "| üí° Feature Selection Built-in | Automatically picks important features |\n",
        "| üìä Works for Classification & Regression | Predict categories or numeric values |\n",
        "\n",
        "---\n",
        "\n",
        "## üíº What Can It Solve?\n",
        "\n",
        "- Spam detection (Yes/No)  \n",
        "- Loan approval (Approve/Reject)  \n",
        "- Disease prediction (Positive/Negative)  \n",
        "- Price prediction (in case of regression)  \n",
        "- Weather-based decisions (like our example)\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Working Step-by-Step\n",
        "\n",
        "1. **Calculate Entropy** ‚Äì Measures impurity in the data.  \n",
        "2. **Find Information Gain** ‚Äì Best feature to split on.  \n",
        "3. **Split the data** based on feature values.  \n",
        "4. **Repeat recursively** for each branch.  \n",
        "5. **Stop** when data is pure or no features left.  \n",
        "6. **Build the tree structure** in nested dictionaries.\n"
      ],
      "metadata": {
        "id": "656Dk1VQuVoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "data = [\n",
        "    ['Sunny', 'Hot', 'No'],\n",
        "    ['Sunny', 'Hot', 'No'],\n",
        "    ['Sunny', 'Mild', 'Yes'],\n",
        "    ['Sunny', 'Cool', 'Yes'],\n",
        "    ['Overcast', 'Hot', 'Yes'],\n",
        "    ['Overcast', 'Mild', 'Yes'],\n",
        "    ['Overcast', 'Cool', 'Yes'],\n",
        "    ['Rainy', 'Hot', 'No'],\n",
        "    ['Rainy', 'Mild', 'Yes'],\n",
        "    ['Rainy', 'Cool', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'Yes'],\n",
        "    ['Rainy', 'Mild', 'Yes'],\n",
        "    ['Overcast', 'Mild', 'Yes'],\n",
        "    ['Rainy', 'Hot', 'No'],\n",
        "    ['Sunny', 'Cool', 'Yes']\n",
        "]\n",
        "features = ['Outlook', 'Temp']\n"
      ],
      "metadata": {
        "id": "a9ZF6cFmi81B"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Python Code (With Explanation)\n",
        "\n",
        "### 1. üìå Entropy Function\n"
      ],
      "metadata": {
        "id": "-ADZHbMiuhBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(data):\n",
        "    labels = [row[-1] for row in data]  # last column = class label\n",
        "    total = len(labels)\n",
        "    label_counts = Counter(labels)\n",
        "\n",
        "    ent = 0.0\n",
        "    step_by_step = []\n",
        "\n",
        "    for label in label_counts:\n",
        "        p = label_counts[label] / total\n",
        "        ent_component = -p * math.log2(p)\n",
        "        step_by_step.append(f\"Label = {label}, Count = {label_counts[label]}, \"\n",
        "                            f\"p = {p:.2f}, -p*log2(p) = {ent_component:.4f}\")\n",
        "        ent += ent_component\n",
        "\n",
        "    return ent, step_by_step"
      ],
      "metadata": {
        "id": "UYciosIJug0T"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úî Purpose:\n",
        "\n",
        "Calculates how mixed the labels are.  \n",
        "Entropy is highest when data is 50-50 between \"Yes\" and \"No\".  \n",
        "Lower entropy means purer data.\n"
      ],
      "metadata": {
        "id": "l_HfusTLuqt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Entropy?\n",
        "\n",
        "Entropy measures uncertainty or impurity in the data.\n",
        "\n",
        "## Formula for Entropy\n",
        "\n",
        "Let \\( H \\) be entropy with classes \\( c \\):\n",
        "\n",
        "$$\n",
        "H = - \\sum_{i=1}^{c} p_i \\log_2 (p_i)\n",
        "$$\n",
        "\n",
        "where \\( p_i \\) is the probability of class \\( i \\).\n",
        "\n",
        "## Example Calculation\n",
        "\n",
        "Suppose our dataset labels are:\n",
        "\n",
        "| Label | Count | Probability \\( p \\) |\n",
        "|-------|-------|---------------------|\n",
        "| Yes   | 10    | \\( \\frac{10}{15} = 0.67 \\) |\n",
        "| No    | 5     | \\( \\frac{5}{15} = 0.33 \\)  |\n",
        "\n",
        "Entropy:\n",
        "\n",
        "$$\n",
        "H = - (0.67 \\times \\log_2 0.67) - (0.33 \\times \\log_2 0.33) \\approx 0.92\n",
        "$$\n"
      ],
      "metadata": {
        "id": "1Gl6hpSuz5K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Entropy of whole dataset"
      ],
      "metadata": {
        "id": "w6NV7Ozc0MF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_value, entropy_steps = entropy(data)\n",
        "print(\"### Entropy of entire dataset:\")\n",
        "print(f\"Entropy = {entropy_value:.4f}\\n\")\n",
        "print(\"Step-by-step calculation:\")\n",
        "for step in entropy_steps:\n",
        "    print(step)\n",
        "# print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5il66hur0J_O",
        "outputId": "96989893-0a87-491e-ada8-93380fc8089f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Entropy of entire dataset:\n",
            "Entropy = 0.8366\n",
            "\n",
            "Step-by-step calculation:\n",
            "Label = No, Count = 4, p = 0.27, -p*log2(p) = 0.5085\n",
            "Label = Yes, Count = 11, p = 0.73, -p*log2(p) = 0.3281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. üìå Split Data Function"
      ],
      "metadata": {
        "id": "meP-bRRCutPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, column, value):\n",
        "    split = []\n",
        "    for row in data:\n",
        "        if row[column] == value:\n",
        "            reduced_row = row[:column] + row[column+1:]  # remove split feature\n",
        "            split.append(reduced_row)\n",
        "    return split\n"
      ],
      "metadata": {
        "id": "2o8Pv9TktXeO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úî Purpose:\n",
        "\n",
        "Filters the data where the given column equals the value  \n",
        "and removes that column from the row (so we don‚Äôt use it again).\n"
      ],
      "metadata": {
        "id": "LIVwqtNcu4-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does it do?\n",
        "\n",
        "- Filters rows where `data[row][column] == value`.\n",
        "- Removes the split feature column (because it's already used).\n",
        "- Returns the subset for further splits.\n",
        "\n",
        "**Example: Split where Outlook = 'Sunny'**\n",
        "\n",
        "- Original row: `['Sunny', 'Hot', 'No']`\n",
        "- Split removes 'Sunny' column ‚Üí `['Hot', 'No']`\n"
      ],
      "metadata": {
        "id": "ovvnTAsW0Y6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. üìå . Information Gain and Best Split\n"
      ],
      "metadata": {
        "id": "bnfUhIyAvEO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(data):\n",
        "    base_entropy, _ = entropy(data)\n",
        "    best_gain = 0\n",
        "    best_column = -1\n",
        "    n_features = len(data[0]) - 1\n",
        "\n",
        "    print(\"### Information Gain Calculation for each feature:\")\n",
        "    for col in range(n_features):\n",
        "        values = set(row[col] for row in data)\n",
        "        new_entropy = 0\n",
        "\n",
        "        print(f\"\\n- Evaluating feature '{features[col]}':\")\n",
        "        for val in values:\n",
        "            subset = split_data(data, col, val)\n",
        "            p = len(subset) / len(data)\n",
        "            subset_entropy, _ = entropy(subset)\n",
        "            weighted_entropy = p * subset_entropy\n",
        "            new_entropy += weighted_entropy\n",
        "\n",
        "            print(f\"  * Value '{val}': subset size = {len(subset)}, p = {p:.2f}, \"\n",
        "                  f\"entropy = {subset_entropy:.4f}, weighted entropy = {weighted_entropy:.4f}\")\n",
        "\n",
        "        info_gain = base_entropy - new_entropy\n",
        "        print(f\"Information Gain for '{features[col]}': {info_gain:.4f}\")\n",
        "\n",
        "        if info_gain > best_gain:\n",
        "            best_gain = info_gain\n",
        "            best_column = col\n",
        "\n",
        "    print(f\"\\nBest feature to split on: '{features[best_column]}' with info gain = {best_gain:.4f}\\n\")\n",
        "    return best_column\n"
      ],
      "metadata": {
        "id": "zqwXLsBPuzP5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úî Purpose:\n",
        "Finds the best feature (column) to split on by calculating information gain for each one. Higher info gain = better split."
      ],
      "metadata": {
        "id": "zTwT2W5ZvIUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Information Gain?\n",
        "\n",
        "Information Gain is the reduction in entropy due to splitting on a feature.\n",
        "\n",
        "$$\n",
        "IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
        "$$\n",
        "\n",
        "- \\( H(S) \\): entropy of the full dataset.  \n",
        "- \\( S_v \\): subset of data where feature \\( A \\) has value \\( v \\).  \n",
        "- \\( \\frac{|S_v|}{|S|} \\): proportion of subset size.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-step:\n",
        "\n",
        "For each feature (column):\n",
        "\n",
        "1. Calculate unique values of that feature.\n",
        "2. For each value:\n",
        "   - Create subset with that value.\n",
        "   - Calculate entropy of subset.\n",
        "   - Calculate weighted entropy = subset size ratio \\(\\times\\) subset entropy.\n",
        "3. Sum weighted entropy of all subsets.\n",
        "4. Calculate information gain = base entropy ‚àí weighted entropy sum.\n",
        "5. Track the feature with max information gain.\n"
      ],
      "metadata": {
        "id": "oJuVrdiV1ihs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QRSGwPfy1NW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. üìå Build Tree Function"
      ],
      "metadata": {
        "id": "7O8fw1I6vLrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(data, features):\n",
        "    labels = [row[-1] for row in data]\n",
        "\n",
        "    if labels.count(labels[0]) == len(labels):\n",
        "        return labels[0]\n",
        "\n",
        "    if len(data[0]) == 1:\n",
        "        return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "    best_feat = best_split(data)\n",
        "    best_feat_name = features[best_feat]\n",
        "\n",
        "    tree = {best_feat_name: {}}\n",
        "    values = set(row[best_feat] for row in data)\n",
        "\n",
        "    for val in values:\n",
        "        subset = split_data(data, best_feat, val)\n",
        "        sub_features = features[:best_feat] + features[best_feat+1:]\n",
        "        subtree = build_tree(subset, sub_features)\n",
        "        tree[best_feat_name][val] = subtree\n",
        "\n",
        "    return tree\n"
      ],
      "metadata": {
        "id": "5ZCBZidQvGea"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úî Purpose:\n",
        "\n",
        "Builds the decision tree recursively:\n",
        "\n",
        "- **Base case:** If all labels are same ‚Üí return result.  \n",
        "- Otherwise, find best feature, split data, and build branches.  \n",
        "- Tree is stored as a dictionary.\n"
      ],
      "metadata": {
        "id": "ka_t9RMHvUeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logic\n",
        "\n",
        "- If all labels are the same, return the label (leaf node).\n",
        "- If no features left, return majority label.\n",
        "- Else:\n",
        "  1. Find best feature to split on.\n",
        "  2. For each value of that feature:\n",
        "     - Split dataset.\n",
        "     - Recursively build subtree.\n",
        "  3. Return tree node as dict.\n"
      ],
      "metadata": {
        "id": "Iqf88EuD2KeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. üìå Run & Output Final Tree"
      ],
      "metadata": {
        "id": "XiW9a4d9vji_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree = build_tree(data, features)\n",
        "print(\"### Final Decision Tree:\")\n",
        "print(decision_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vz2wnXavOg9",
        "outputId": "ae86fea9-1e8d-4e76-b95c-33a05a00c829"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Information Gain Calculation for each feature:\n",
            "\n",
            "- Evaluating feature 'Outlook':\n",
            "  * Value 'Overcast': subset size = 4, p = 0.27, entropy = 0.0000, weighted entropy = 0.0000\n",
            "  * Value 'Sunny': subset size = 6, p = 0.40, entropy = 0.9183, weighted entropy = 0.3673\n",
            "  * Value 'Rainy': subset size = 5, p = 0.33, entropy = 0.9710, weighted entropy = 0.3237\n",
            "Information Gain for 'Outlook': 0.1457\n",
            "\n",
            "- Evaluating feature 'Temp':\n",
            "  * Value 'Cool': subset size = 4, p = 0.27, entropy = 0.0000, weighted entropy = 0.0000\n",
            "  * Value 'Hot': subset size = 5, p = 0.33, entropy = 0.7219, weighted entropy = 0.2406\n",
            "  * Value 'Mild': subset size = 6, p = 0.40, entropy = 0.0000, weighted entropy = 0.0000\n",
            "Information Gain for 'Temp': 0.5960\n",
            "\n",
            "Best feature to split on: 'Temp' with info gain = 0.5960\n",
            "\n",
            "### Information Gain Calculation for each feature:\n",
            "\n",
            "- Evaluating feature 'Outlook':\n",
            "  * Value 'Overcast': subset size = 1, p = 0.20, entropy = 0.0000, weighted entropy = 0.0000\n",
            "  * Value 'Sunny': subset size = 2, p = 0.40, entropy = 0.0000, weighted entropy = 0.0000\n",
            "  * Value 'Rainy': subset size = 2, p = 0.40, entropy = 0.0000, weighted entropy = 0.0000\n",
            "Information Gain for 'Outlook': 0.7219\n",
            "\n",
            "Best feature to split on: 'Outlook' with info gain = 0.7219\n",
            "\n",
            "### Final Decision Tree:\n",
            "{'Temp': {'Cool': 'Yes', 'Hot': {'Outlook': {'Overcast': 'Yes', 'Sunny': 'No', 'Rainy': 'No'}}, 'Mild': 'Yes'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úî Output:\n",
        "Will print a dictionary representing the final decision tree structure."
      ],
      "metadata": {
        "id": "I6e8BCQDvnfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install graphviz\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "def add_nodes_edges(tree, dot=None, parent=None):\n",
        "    if dot is None:\n",
        "        dot = Digraph()\n",
        "        dot.attr('node', shape='ellipse', fontsize='10')\n",
        "\n",
        "    for node, branches in tree.items():\n",
        "        # Add current node\n",
        "        dot.node(node)\n",
        "\n",
        "        if parent:\n",
        "            dot.edge(parent, node)\n",
        "\n",
        "        if isinstance(branches, dict):\n",
        "            for edge_label, subtree in branches.items():\n",
        "                # Create a unique id for subtree nodes to avoid collisions\n",
        "                if isinstance(subtree, dict):\n",
        "                    # subtree is another dict - create new node with a random name\n",
        "                    sub_node = f\"{node}_{edge_label}\"\n",
        "                    dot.node(sub_node, label=edge_label)\n",
        "                    dot.edge(node, sub_node, label=edge_label)\n",
        "                    # Recursive call\n",
        "                    add_nodes_edges(subtree, dot, sub_node)\n",
        "                else:\n",
        "                    # Leaf node\n",
        "                    leaf_name = f\"{node}_{edge_label}_{subtree}\"\n",
        "                    dot.node(leaf_name, label=subtree, shape='box', style='filled', color='lightgrey')\n",
        "                    dot.edge(node, leaf_name, label=edge_label)\n",
        "        else:\n",
        "            # Leaf node (should not occur here usually)\n",
        "            dot.node(branches)\n",
        "            if parent:\n",
        "                dot.edge(parent, branches)\n",
        "\n",
        "    return dot\n",
        "\n",
        "# Use this function on your tree:\n",
        "\n",
        "dot = add_nodes_edges(decision_tree)\n",
        "dot.render('decision_tree_graph', view=True)  # This saves and opens the graph PDF file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5-fxtC9Ayea1",
        "outputId": "ed4b0da2-bc6d-427c-ff26-f43f16606996"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'decision_tree_graph.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AIqWGN1vlsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n1sP9NznxeYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå What is a Random Forest?\n",
        "\n",
        "A Random Forest is an ensemble of many decision trees working together to make a better, more accurate prediction.\n",
        "\n",
        "Instead of relying on one decision tree, it builds multiple trees on random samples of the data and features, then combines their results by voting.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why Use a Random Forest?\n",
        "\n",
        "| Reason            | Description                                                                                  |\n",
        "|-------------------|----------------------------------------------------------------------------------------------|\n",
        "| üå≤ Multiple Trees  | Reduces the chance of overfitting (too much fitting to training data) by averaging many trees. |\n",
        "| üé≤ Randomness      | Each tree trains on a random subset of data and features, adding diversity.                   |\n",
        "| ü§ù Better Accuracy | Combining many trees usually gives better predictions than a single tree.                     |\n",
        "| üí™ Handles Overfitting | More robust than a single decision tree, less prone to mistakes on new data.               |\n",
        "| üë©‚Äçüíª Easy to Use   | Works well with minimal tuning and handles both classification and regression.                |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° What Can It Solve?\n",
        "\n",
        "- Email spam detection (spam or not)  \n",
        "- Credit risk evaluation (safe or risky loan)  \n",
        "- Medical diagnosis (disease or no disease)  \n",
        "- Stock price prediction (continuous value)  \n",
        "- Weather forecasting (rain/no rain)  \n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ How Does Random Forest Work Step-by-Step?\n",
        "\n",
        "1. **Bootstrap Sampling:** Create multiple random samples (with replacement) from the training data.  \n",
        "2. **Build Decision Trees:** For each sample, build a decision tree using a random subset of features at each split.  \n",
        "3. **Make Predictions:** To predict, each tree gives its own result.  \n",
        "4. **Majority Voting:** Combine predictions from all trees ‚Äî the most common prediction wins (for classification).  \n",
        "5. **Output:** Final prediction is usually more accurate and stable than a single decision tree.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "Random Forest = many decision trees + randomness + voting.\n",
        "\n",
        "It reduces errors and overfitting by averaging multiple trees.\n",
        "\n",
        "Widely used because it‚Äôs simple, effective, and powerful.\n"
      ],
      "metadata": {
        "id": "Blyc3hQD5jgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Entropy calculation function (same as before)\n",
        "def entropy(data):\n",
        "    labels = [row[-1] for row in data]  # Extract class labels (last element in each row)\n",
        "    total = len(labels)                  # Total number of samples\n",
        "    counts = Counter(labels)             # Count how many times each label occurs\n",
        "\n",
        "    ent = 0.0                           # Initialize entropy\n",
        "\n",
        "    for label in counts:\n",
        "        p = counts[label] / total       # Probability of label\n",
        "        ent -= p * math.log2(p)         # Entropy formula: sum(-p*log2(p))\n",
        "    return ent\n",
        "\n",
        "\n",
        "# 2. Split data based on feature and value\n",
        "def split_data(data, column, value):\n",
        "    split = []\n",
        "    for row in data:\n",
        "        if row[column] == value:\n",
        "            # Remove the feature column because we split on it\n",
        "            reduced_row = row[:column] + row[column+1:]\n",
        "            split.append(reduced_row)\n",
        "    return split\n",
        "\n",
        "\n",
        "# 3. Build one decision tree (simplified, no pruning)\n",
        "def build_tree(data, features):\n",
        "    labels = [row[-1] for row in data]\n",
        "\n",
        "    if labels.count(labels[0]) == len(labels):\n",
        "        return labels[0]          # If all labels same, return that label (leaf node)\n",
        "\n",
        "    if len(data[0]) == 1:\n",
        "        return Counter(labels).most_common(1)[0][0]  # If no more features, return majority label\n",
        "\n",
        "    base_entropy = entropy(data)\n",
        "    best_gain = 0\n",
        "    best_col = -1\n",
        "    best_vals = set()\n",
        "\n",
        "    n_features = len(features)\n",
        "\n",
        "    # Select random subset of features (half) for randomness (random forest concept)\n",
        "    features_idx = random.sample(range(n_features), max(1, n_features // 2))\n",
        "\n",
        "    for col in features_idx:\n",
        "        values = set(row[col] for row in data)   # Unique values of feature in dataset\n",
        "        new_entropy = 0\n",
        "\n",
        "        for val in values:\n",
        "            subset = split_data(data, col, val)\n",
        "            p = len(subset) / len(data)          # Weight of subset\n",
        "            new_entropy += p * entropy(subset)   # Weighted entropy\n",
        "\n",
        "        info_gain = base_entropy - new_entropy     # Information gain by splitting on feature\n",
        "\n",
        "        if info_gain > best_gain:\n",
        "            best_gain = info_gain\n",
        "            best_col = col\n",
        "            best_vals = values\n",
        "\n",
        "    if best_col == -1:\n",
        "        return Counter(labels).most_common(1)[0][0]  # No good split, return majority label\n",
        "\n",
        "    tree = {features[best_col]: {}}\n",
        "    for val in best_vals:\n",
        "        subset = split_data(data, best_col, val)\n",
        "        sub_features = features[:best_col] + features[best_col+1:]  # Remove used feature\n",
        "        tree[features[best_col]][val] = build_tree(subset, sub_features)  # Recursive call\n",
        "\n",
        "    return tree\n",
        "\n",
        "\n",
        "# 4. Bootstrap sample (random sample with replacement)\n",
        "def bootstrap_sample(data):\n",
        "    n = len(data)\n",
        "    return [random.choice(data) for _ in range(n)]\n",
        "\n",
        "# 5. Predict for one tree\n",
        "def predict(tree, features, sample):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree          # Leaf node reached, return label\n",
        "\n",
        "    root = next(iter(tree))  # Get root feature to split on\n",
        "    feature_val = sample[features.index(root)]  # Value of feature in sample\n",
        "    subtree = tree[root].get(feature_val)       # Traverse down the branch\n",
        "\n",
        "    if subtree is None:\n",
        "        return None        # If no branch for value, return None\n",
        "\n",
        "    # Remove used feature and recurse\n",
        "    return predict(subtree, [f for f in features if f != root],\n",
        "                   sample[:features.index(root)] + sample[features.index(root)+1:])\n",
        "\n",
        "# 6. Random Forest predict - majority vote\n",
        "def random_forest_predict(trees, features, sample):\n",
        "    predictions = []\n",
        "    for tree in trees:\n",
        "        pred = predict(tree, features, sample)\n",
        "        if pred is not None:\n",
        "            predictions.append(pred)\n",
        "\n",
        "    if predictions:\n",
        "        # Return most common prediction\n",
        "        return Counter(predictions).most_common(1)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# 7. Train random forest\n",
        "def train_random_forest(data, features, n_trees=5):\n",
        "    trees = []\n",
        "    for _ in range(n_trees):\n",
        "        sample = bootstrap_sample(data)       # Random subset with replacement\n",
        "        tree = build_tree(sample, features)   # Build one tree\n",
        "        trees.append(tree)\n",
        "    return trees\n",
        "\n",
        "\n",
        "# ====== Example usage ======\n",
        "\n",
        "data = [\n",
        "    ['Sunny', 'Hot', 'No'],\n",
        "    ['Sunny', 'Hot', 'No'],\n",
        "    ['Sunny', 'Mild', 'Yes'],\n",
        "    ['Sunny', 'Cool', 'Yes'],\n",
        "    ['Overcast', 'Hot', 'Yes'],\n",
        "    ['Overcast', 'Mild', 'Yes'],\n",
        "    ['Overcast', 'Cool', 'Yes'],\n",
        "    ['Rainy', 'Hot', 'No'],\n",
        "    ['Rainy', 'Mild', 'Yes'],\n",
        "    ['Rainy', 'Cool', 'Yes'],\n",
        "]\n",
        "\n",
        "features = ['Outlook', 'Temp']\n",
        "\n",
        "# Train forest\n",
        "forest = train_random_forest(data, features, n_trees=5)\n",
        "\n",
        "# Predict example sample\n",
        "sample = ['Sunny', 'Cool']\n",
        "prediction = random_forest_predict(forest, features, sample)\n",
        "print(f\"Prediction for {sample} = {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWiO4R5Uyg4c",
        "outputId": "96f30461-aa44-405e-bdb5-4964a7c60ad9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for ['Sunny', 'Cool'] = Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def add_nodes_edges(tree, dot=None, parent=None):\n",
        "    if dot is None:\n",
        "        dot = Digraph()\n",
        "        dot.attr('node', shape='box')\n",
        "\n",
        "    if not isinstance(tree, dict):\n",
        "        # Leaf node\n",
        "        node_id = str(id(tree))\n",
        "        dot.node(node_id, str(tree), shape='ellipse', style='filled', color='lightgrey')\n",
        "        if parent:\n",
        "            dot.edge(parent, node_id)\n",
        "        return dot\n",
        "\n",
        "    # Internal node\n",
        "    root = next(iter(tree))\n",
        "    node_id = str(id(tree))\n",
        "    dot.node(node_id, root)\n",
        "\n",
        "    if parent:\n",
        "        dot.edge(parent, node_id)\n",
        "\n",
        "    for val, subtree in tree[root].items():\n",
        "        val_id = f\"{node_id}_{val}\"\n",
        "        dot.node(val_id, str(val), shape='oval', style='filled', color='lightblue')\n",
        "        dot.edge(node_id, val_id)\n",
        "        add_nodes_edges(subtree, dot, val_id)\n",
        "\n",
        "    return dot\n"
      ],
      "metadata": {
        "id": "c4zKtMvI3s1S"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize first tree in forest\n",
        "dot = add_nodes_edges(forest[0])\n",
        "dot.render('decision_tree', format='pdf', cleanup=True)  # Saves decision_tree.png file\n",
        "print(\"Decision tree graph saved as decision_tree.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTnup5qG31DG",
        "outputId": "4d1b00c7-5e39-4e81-f5a2-8bb50d6b7bca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree graph saved as decision_tree.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlC917pI32Ye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}